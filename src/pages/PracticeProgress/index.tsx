import React, { useState, useRef, useEffect, useCallback } from "react";
import { useNavigate } from "react-router-dom";
import * as S from "./style";
import { ServerMessage } from "../../types/practice";
import elderlyMen from "../../assets/elderly_men.png";
import { useUser } from "../../store/UserContext";

const PracticeProgress = () => {
  // ì‚¬ìš©ì ì •ë³´
  const { user } = useUser();
  
  // ìƒíƒœ ê´€ë¦¬
  const [isConnected, setIsConnected] = useState(false);
  const [isPlaying, setIsPlaying] = useState(false);
  const [isPracticeStarted, setIsPracticeStarted] = useState(false);
  const [isTTSPlaying, setIsTTSPlaying] = useState(false);
  type ActivityStatus = "listening" | "processing" | "talking" | "idle";
  const [activityStatus, setActivityStatus] = useState<ActivityStatus>("idle");
  const [timer, setTimer] = useState(0);
  const [statusMessage, setStatusMessage] = useState("ëŒ€ê¸° ì¤‘");
  const [patientName, setPatientName] = useState("í™˜ì");
  const [conversation, setConversation] = useState<
    { speaker_role: "doctor" | "patient"; text: string }[]
  >([]);
  const navigate = useNavigate();

  // Ref ê´€ë¦¬
  const timerInterval = useRef<NodeJS.Timeout | null>(null);
  const websocket = useRef<WebSocket | null>(null);
  const audioContext = useRef<AudioContext | null>(null);
  const audioStream = useRef<MediaStream | null>(null);
  const audioSource = useRef<MediaStreamAudioSourceNode | null>(null);
  const audioWorkletNode = useRef<AudioWorkletNode | null>(null);
  const audioPlayer = useRef<HTMLAudioElement | null>(null);
  const audioQueue = useRef<string[]>([]);

  const processAudioQueue = useCallback(async () => {
    if (isPlaying || audioQueue.current.length === 0) {
      return;
    }

    setActivityStatus("talking");
    setIsPlaying(true);
    setIsTTSPlaying(true);
    
    if (audioSource.current && audioWorkletNode.current) {
      try {
        audioSource.current.disconnect(audioWorkletNode.current);
        console.log("ğŸ¤ ë§ˆì´í¬ ì…ë ¥ ì¼ì‹œ ì°¨ë‹¨ (TTS ì¬ìƒ ì¤‘)");
      } catch (error) {
        console.error("ì˜¤ë””ì˜¤ ì—°ê²° í•´ì œ ì˜¤ë¥˜:", error);
      }
    }
    
    if (audioWorkletNode.current) {
      audioWorkletNode.current.port.postMessage({
        command: "setTTSState",
        isPlaying: true
      });
    }

    const audioData = audioQueue.current.shift();

    if (audioData && audioPlayer.current) {
      try {
        console.log(`ğŸ”Š TTS Base64 ì˜¤ë””ì˜¤ ìˆ˜ì‹ : ${audioData.length} ë¬¸ì`);
        
        const binaryString = atob(audioData);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }
        const audioBlob = new Blob([bytes], { type: 'audio/mpeg' });
        
        const objectUrl = URL.createObjectURL(audioBlob);
        audioPlayer.current.src = objectUrl;
        
        console.log("ğŸ”Š TTS ìŒì„± ì¬ìƒ ì‹œì‘ (ë©”ëª¨ë¦¬ ë²„í¼)");
        await audioPlayer.current.play();
        
        audioPlayer.current.onended = async () => {
          console.log("âœ… TTS ìŒì„± ì¬ìƒ ì™„ë£Œ");
          URL.revokeObjectURL(objectUrl);
          
          await new Promise(resolve => setTimeout(resolve, 500));
          
          if (audioSource.current && audioWorkletNode.current) {
            try {
              audioSource.current.connect(audioWorkletNode.current);
              console.log("ğŸ¤ ë§ˆì´í¬ ì…ë ¥ ì¬ê°œ");
            } catch (error) {
              console.error("ì˜¤ë””ì˜¤ ì—°ê²° ì¬ê°œ ì˜¤ë¥˜:", error);
            }
          }
          
          if (audioWorkletNode.current) {
            audioWorkletNode.current.port.postMessage({
              command: "setTTSState",
              isPlaying: false
            });
          }
          
          setIsTTSPlaying(false);
          setIsPlaying(false);
          setActivityStatus("listening");
        };
      } catch (error) {
        console.error("TTS ì¬ìƒ ì˜¤ë¥˜:", error);
        setIsTTSPlaying(false);
        setIsPlaying(false);
        setActivityStatus("listening");
      }
    } else {
      setIsTTSPlaying(false);
      setIsPlaying(false);
      setActivityStatus("listening");
    }
  }, [isPlaying]);

  // ì›¹ì†Œì¼“ ì—°ê²° ë° í•´ì œ
  const connectWebSocket = useCallback(() => {
    const userId = user?.id;
    
    const wsUrl = `${process.env.REACT_APP_WEBSOCKET_URL}/ws/${userId}`;

    if (websocket.current) {
      websocket.current.close();
    }

    const ws = new WebSocket(wsUrl);

    ws.onopen = () => {
      console.log("WebSocket connected");
      setIsConnected(true);
      setStatusMessage("ì„œë²„ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.");
    };

    ws.onmessage = (event) => {
      const message: ServerMessage = JSON.parse(event.data);
      console.log("Received message:", message);
      handleServerMessage(message);
    };

    ws.onclose = () => {
      console.log("WebSocket disconnected");
      setIsConnected(false);
      setStatusMessage("ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.");
    };

    ws.onerror = (error) => {
      console.error("WebSocket error:", error);
      setStatusMessage("ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.");
    };

    websocket.current = ws;
  }, []);

  // ì„œë²„ ë©”ì‹œì§€ ì²˜ë¦¬
  const handleServerMessage = (message: ServerMessage) => {
    setStatusMessage(message.message || statusMessage);

    switch (message.type) {
      case "listening":
      case "no_speech":
        setActivityStatus("listening");
        break;
      case "processing":
        setActivityStatus("processing");
        break;
      case "session_started":
        setPatientName(message.scenario_name || "í™˜ì");
        break;
      case "voice_response":
        if (message.user_text) {
          setConversation((prev) => [
            ...prev,
            { speaker_role: "doctor", text: message.user_text! },
          ]);
        }
        if (message.ai_text) {
          setConversation((prev) => [
            ...prev,
            { speaker_role: "patient", text: message.ai_text! },
          ]);
        }
        if (message.tts_audio_base64) {
          audioQueue.current.push(message.tts_audio_base64);
          processAudioQueue();
        }
        break;
      case "conversation_ended":
        if (audioSource.current && audioWorkletNode.current) {
          audioSource.current.disconnect(audioWorkletNode.current);
        }
        // ì„¸ì…˜ ì¢…ë£Œ ì²˜ë¦¬
        break;
    }
  };

  // ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì´ˆê¸°í™”
  const initAudio = useCallback(async () => {
    try {
      if (!audioContext.current) {
        audioContext.current = new AudioContext({ sampleRate: 16000 });
        await audioContext.current.audioWorklet.addModule("/audioProcessor.js");
      }

      if (audioContext.current.state === "suspended") {
        await audioContext.current.resume();
      }

      if (!audioStream.current) {
        audioStream.current = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
          },
        });
      }

      if (!audioSource.current) {
        audioSource.current = audioContext.current.createMediaStreamSource(
          audioStream.current
        );
      }

      if (!audioWorkletNode.current) {
        audioWorkletNode.current = new AudioWorkletNode(
          audioContext.current,
          "audio-processor"
        );

        audioWorkletNode.current.port.onmessage = (event) => {
          if (websocket.current?.readyState === WebSocket.OPEN) {
            websocket.current.send(event.data);
          }
        };
      }
    } catch (error) {
      console.error("Error initializing audio:", error);
      setStatusMessage("ë§ˆì´í¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.");
    }
  }, []);

  // ì‹¤ìŠµ ì‹œì‘/ì¢…ë£Œ í•¸ë“¤ëŸ¬
  const handleTogglePractice = async () => {
    if (!isPracticeStarted) {
      await initAudio();
      if (audioSource.current && audioWorkletNode.current) {
        audioSource.current.connect(audioWorkletNode.current);
      }
      setActivityStatus("listening");
      setIsPracticeStarted(true);
      timerInterval.current = setInterval(() => {
        setTimer((prev) => prev + 1);
      }, 1000);
    } else {
      if (timerInterval.current) {
        clearInterval(timerInterval.current);
      }
      if (audioSource.current && audioWorkletNode.current) {
        audioSource.current.disconnect(audioWorkletNode.current);
      }
      setActivityStatus("idle");
      setIsPracticeStarted(false);
    }
  };

  const handleSubmit = () => {
    if (timerInterval.current) {
      clearInterval(timerInterval.current);
    }
    if (websocket.current) {
      websocket.current.close();
    }
    // í•„ìš”í•œ ê²½ìš° ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ë“± ë‹¤ë¥¸ ë¦¬ì†ŒìŠ¤ë„ ì •ë¦¬
    if (audioContext.current && audioContext.current.state !== "closed") {
      audioContext.current.close();
    }
    navigate("/result");
  };

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸/ì–¸ë§ˆìš´íŠ¸ ì‹œ íš¨ê³¼
  useEffect(() => {
    connectWebSocket();
    audioPlayer.current = new Audio();

    return () => {
      websocket.current?.close();
      audioStream.current?.getTracks().forEach((track) => track.stop());
      if (audioContext.current && audioContext.current.state !== "closed") {
        audioContext.current.close();
      }
    };
  }, [connectWebSocket]);

  // ë…¹ìŒ ìƒíƒœ ë³€ê²½ ì‹œ íš¨ê³¼
  useEffect(() => {
    if (!isPlaying && isPracticeStarted) {
      if (audioSource.current && audioWorkletNode.current) {
        audioSource.current.connect(audioWorkletNode.current);
      }
    }
  }, [isPlaying, isPracticeStarted]);

  useEffect(() => {
    if (!isPlaying) {
      processAudioQueue();
    }
  }, [isPlaying, processAudioQueue]);

  const formatTime = (seconds: number) => {
    const minutes = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${minutes.toString().padStart(2, "0")}:${secs
      .toString()
      .padStart(2, "0")}`;
  };

  return (
    <S.Container>
      <S.ControlSection>
        <S.Timer>{formatTime(timer)}</S.Timer>
        <S.Button onClick={handleTogglePractice} active={isPracticeStarted}>
          {isPracticeStarted ? "ì‹¤ìŠµ ì¤‘ë‹¨" : "ì‹¤ìŠµ ì‹œì‘"}
        </S.Button>
        <S.SubmitButton onClick={handleSubmit}>âœ… ì œì¶œ</S.SubmitButton>
      </S.ControlSection>
      <S.PracticeArea>
        <S.PatientVideoArea>
          <S.PatientAvatar>
            <img
              src={elderlyMen}
              alt="Patient"
              style={{ width: "100%", height: "100%", objectFit: "cover" }}
            />
          </S.PatientAvatar>
          <S.PatientName>{patientName}</S.PatientName>
          <S.StatusBadge>
            {!isConnected
              ? "ğŸ”´ ì—°ê²° ëŠê¹€"
              : !isPracticeStarted
              ? "ğŸŸ¡ ëŒ€ê¸° ì¤‘"
              : activityStatus === "listening"
              ? "ğŸŸ¢ ë“£ëŠ” ì¤‘"
              : activityStatus === "processing"
              ? "ğŸ”µ ë¶„ì„ ì¤‘"
              : "ğŸ—£ï¸ ë§í•˜ëŠ” ì¤‘"}
          </S.StatusBadge>
        </S.PatientVideoArea>
        <S.InfoPanel>
          <S.MemoCard>
            <S.CardHeader>
              <span>âœï¸</span>
              <span>ë©”ëª¨ì¥</span>
            </S.CardHeader>
            <S.MemoArea
              height="200px"
              placeholder="ì´ê³³ì— ë©”ëª¨ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
            />
          </S.MemoCard>
          {/* í™•ì¸ìš© ëŒ€í™”ë‚´ìš© ë¡œê·¸ ì¶”í›„ì— ì‚­ì œ */}
          {/* <S.MemoCard>
            <S.CardHeader>
              <span>âœï¸</span>
              <span>ëŒ€í™” ë‚´ìš© -ì‚­ì œì˜ˆì •</span>
            </S.CardHeader>
            <S.NotesArea>
              {conversation.map((entry, index) => (
                <div key={index}>
                  <strong>
                    {entry.speaker_role === "doctor" ? "ë‚˜: " : "í™˜ì: "}
                  </strong>
                  {entry.text}
                </div>
              ))}
            </S.NotesArea>
          </S.MemoCard> */}
        </S.InfoPanel>
      </S.PracticeArea>
    </S.Container>
  );
};

export default PracticeProgress;
