import React, { useState, useRef, useEffect, useCallback } from "react";
import { useNavigate } from "react-router-dom";
import * as S from "./style";
import { ServerMessage } from "../../types/practice";

const PracticeProgress = () => {
  // ìƒíƒœ ê´€ë¦¬
  const [isConnected, setIsConnected] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [isPlaying, setIsPlaying] = useState(false);
  const [isPracticeStarted, setIsPracticeStarted] = useState(false);
  const [timer, setTimer] = useState(0);
  const [statusMessage, setStatusMessage] = useState("ëŒ€ê¸° ì¤‘");
  const [patientName, setPatientName] = useState("í™˜ì");
  const [conversation, setConversation] = useState<
    { speaker: "user" | "ai"; text: string }[]
  >([]);
  const navigate = useNavigate();

  // Ref ê´€ë¦¬
  const timerInterval = useRef<NodeJS.Timeout | null>(null);
  const websocket = useRef<WebSocket | null>(null);
  const audioContext = useRef<AudioContext | null>(null);
  const audioStream = useRef<MediaStream | null>(null);
  const audioSource = useRef<MediaStreamAudioSourceNode | null>(null);
  const audioWorkletNode = useRef<AudioWorkletNode | null>(null);
  const audioPlayer = useRef<HTMLAudioElement | null>(null);
  const audioQueue = useRef<string[]>([]);

  const processAudioQueue = useCallback(async () => {
    if (isPlaying || audioQueue.current.length === 0) {
      return;
    }

    setIsPlaying(true);
    const audioUrl = audioQueue.current.shift();

    if (audioUrl && audioPlayer.current) {
      try {
        const correctedUrl = audioUrl.replace("/static/audio/", "/cache/tts/");
        const response = await fetch(
          `${process.env.REACT_APP_API_URL}${correctedUrl}`
        );
        const audioBlob = await response.blob();
        const objectUrl = URL.createObjectURL(audioBlob);
        audioPlayer.current.src = objectUrl;
        audioPlayer.current.play();
        audioPlayer.current.onended = () => {
          URL.revokeObjectURL(objectUrl);
          setIsPlaying(false);
        };
      } catch (error) {
        console.error("Error fetching or playing audio:", error);
        setIsPlaying(false);
      }
    } else {
      setIsPlaying(false);
    }
  }, [isPlaying]);

  // ì›¹ì†Œì¼“ ì—°ê²° ë° í•´ì œ
  const connectWebSocket = useCallback(() => {
    const userId = `user_${Date.now()}`; // ì„ì‹œ ì‚¬ìš©ì ID
    const wsUrl = `${process.env.REACT_APP_WEBSOCKET_URL}/ws/${userId}`;

    if (websocket.current) {
      websocket.current.close();
    }

    const ws = new WebSocket(wsUrl);

    ws.onopen = () => {
      console.log("WebSocket connected");
      setIsConnected(true);
      setStatusMessage("ì„œë²„ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.");
    };

    ws.onmessage = (event) => {
      const message: ServerMessage = JSON.parse(event.data);
      console.log("Received message:", message);
      handleServerMessage(message);
    };

    ws.onclose = () => {
      console.log("WebSocket disconnected");
      setIsConnected(false);
      setIsRecording(false);
      setStatusMessage("ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.");
    };

    ws.onerror = (error) => {
      console.error("WebSocket error:", error);
      setStatusMessage("ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.");
    };

    websocket.current = ws;
  }, []);

  // ì„œë²„ ë©”ì‹œì§€ ì²˜ë¦¬
  const handleServerMessage = (message: ServerMessage) => {
    setStatusMessage(message.message || statusMessage);

    switch (message.type) {
      case "session_started":
        setPatientName(message.scenario_name || "í™˜ì");
        break;
      case "voice_response":
        if (message.user_text) {
          setConversation((prev) => [
            ...prev,
            { speaker: "user", text: message.user_text! },
          ]);
        }
        if (message.ai_text) {
          setConversation((prev) => [
            ...prev,
            { speaker: "ai", text: message.ai_text! },
          ]);
        }
        if (message.audio_url) {
          setIsRecording(false); // TTS ìˆ˜ì‹  ì‹œ ë…¹ìŒ ì¤‘ë‹¨
          audioQueue.current.push(message.audio_url);
          processAudioQueue();
        }
        break;
      case "conversation_ended":
        setIsRecording(false);
        // ì„¸ì…˜ ì¢…ë£Œ ì²˜ë¦¬
        break;
    }
  };

  // ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì´ˆê¸°í™”
  const initAudio = useCallback(async () => {
    try {
      if (!audioContext.current) {
        audioContext.current = new AudioContext({ sampleRate: 16000 });
        await audioContext.current.audioWorklet.addModule("/audioProcessor.js");
      }

      if (audioContext.current.state === "suspended") {
        await audioContext.current.resume();
      }

      if (!audioStream.current) {
        audioStream.current = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
          },
        });
      }

      if (!audioSource.current) {
        audioSource.current = audioContext.current.createMediaStreamSource(
          audioStream.current
        );
      }

      if (!audioWorkletNode.current) {
        audioWorkletNode.current = new AudioWorkletNode(
          audioContext.current,
          "audio-processor"
        );

        audioWorkletNode.current.port.onmessage = (event) => {
          if (websocket.current?.readyState === WebSocket.OPEN) {
            websocket.current.send(event.data);
          }
        };
      }
    } catch (error) {
      console.error("Error initializing audio:", error);
      setStatusMessage("ë§ˆì´í¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.");
    }
  }, []);

  // ì‹¤ìŠµ ì‹œì‘/ì¢…ë£Œ í•¸ë“¤ëŸ¬
  const handleStartPractice = () => {
    setIsPracticeStarted(true);
    timerInterval.current = setInterval(() => {
      setTimer((prev) => prev + 1);
    }, 1000);
  };

  const handleToggleRecording = async () => {
    if (!isConnected) {
      connectWebSocket();
      return;
    }

    if (isPlaying) {
      setStatusMessage("í™˜ìê°€ ë§í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...");
      return;
    }

    if (!isRecording) {
      await initAudio();
      setIsRecording(true);
    } else {
      setIsRecording(false);
    }
  };

  const handleSubmit = () => {
    if (timerInterval.current) {
      clearInterval(timerInterval.current);
    }
    setIsRecording(false);
    if (websocket.current) {
      websocket.current.close();
    }
    // í•„ìš”í•œ ê²½ìš° ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ë“± ë‹¤ë¥¸ ë¦¬ì†ŒìŠ¤ë„ ì •ë¦¬
    if (audioContext.current && audioContext.current.state !== "closed") {
      audioContext.current.close();
    }
    navigate("/result");
  };

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸/ì–¸ë§ˆìš´íŠ¸ ì‹œ íš¨ê³¼
  useEffect(() => {
    connectWebSocket();
    audioPlayer.current = new Audio();

    return () => {
      websocket.current?.close();
      audioStream.current?.getTracks().forEach((track) => track.stop());
      if (audioContext.current && audioContext.current.state !== "closed") {
        audioContext.current.close();
      }
    };
  }, [connectWebSocket]);

  // ë…¹ìŒ ìƒíƒœ ë³€ê²½ ì‹œ íš¨ê³¼
  useEffect(() => {
    if (isRecording && audioSource.current && audioWorkletNode.current) {
      audioSource.current.connect(audioWorkletNode.current);
    } else {
      audioSource.current?.disconnect();
    }
  }, [isRecording]);

  useEffect(() => {
    if (!isPlaying) {
      processAudioQueue();
    }
  }, [isPlaying, processAudioQueue]);

  const formatTime = (seconds: number) => {
    const minutes = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${minutes.toString().padStart(2, "0")}:${secs
      .toString()
      .padStart(2, "0")}`;
  };

  return (
    <S.Container>
      <S.ControlSection>
        <S.Timer>{formatTime(timer)}</S.Timer>
        <S.Button onClick={handleStartPractice} disabled={isPracticeStarted}>
          ì‹¤ìŠµ ì‹œì‘
        </S.Button>
        <S.Button
          onClick={handleToggleRecording}
          disabled={!isConnected || !isPracticeStarted}
        >
          {isRecording ? "ëŒ€í™” ì¤‘ì§€" : "ëŒ€í™” ì‹œì‘"}
        </S.Button>
        {/* ì¶”í›„ì— tts ì¶œë ¥ì´ ëë‚˜ê³  ë°”ë¡œ ë§ˆì´í¬ ì…ë ¥ì„ ë°›ê²Œ ëœë‹¤ë©´ 'ì‹¤ìŠµ ì‹œì‘' ë²„íŠ¼ í•˜ë‚˜ë¡œ ë³€ê²½ë  ìˆ˜ ìˆìŒ. */}
        <S.SubmitButton onClick={handleSubmit}>âœ… ì œì¶œ</S.SubmitButton>
      </S.ControlSection>
      <S.PracticeArea>
        <S.PatientVideoArea>
          <S.PatientAvatar>ğŸ‘¨â€ğŸ’¼</S.PatientAvatar>
          <S.PatientName>{patientName}</S.PatientName>
          <S.StatusBadge>
            {isConnected
              ? isRecording
                ? "ğŸŸ¢ ëŒ€í™” ì¤‘"
                : "ğŸŸ¡ ëŒ€ê¸° ì¤‘"
              : "ğŸ”´ ì—°ê²° ëŠê¹€"}
          </S.StatusBadge>
        </S.PatientVideoArea>
        <S.InfoPanel>
          <S.MemoCard>
            <S.CardHeader>
              <span>ğŸ“</span>
              <span>ë©”ëª¨ì¥</span>
            </S.CardHeader>
            <S.MemoArea
              height="200px"
              placeholder="ì´ê³³ì— ë©”ëª¨ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
            />
          </S.MemoCard>
          <S.MemoCard>
            <S.CardHeader>
              <span>ğŸ“œ</span>
              <span>ì§„ì°° ë‚´ì—­</span>
            </S.CardHeader>
            <S.MemoArea height="100px" placeholder="ì§„ì°° ë‚´ì—­ì„ ì…ë ¥í•˜ì„¸ìš”." />
          </S.MemoCard>
          {/* í™•ì¸ìš© ëŒ€í™”ë‚´ìš© ë¡œê·¸ ì¶”í›„ì— ì‚­ì œ */}
          <S.MemoCard>
            <S.CardHeader>
              <span>âœï¸</span>
              <span>ëŒ€í™” ë‚´ìš©</span>
            </S.CardHeader>
            <S.NotesArea>
              {conversation.map((entry, index) => (
                <div key={index}>
                  <strong>
                    {entry.speaker === "user" ? "ë‚˜: " : "í™˜ì: "}
                  </strong>
                  {entry.text}
                </div>
              ))}
            </S.NotesArea>
          </S.MemoCard>
        </S.InfoPanel>
      </S.PracticeArea>
    </S.Container>
  );
};

export default PracticeProgress;
