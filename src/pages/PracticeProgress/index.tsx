import React, { useState, useRef, useEffect, useCallback } from "react";
import * as S from "./style";

// ì˜¤ë””ì˜¤ ì²˜ë¦¬ ê´€ë ¨ íƒ€ì… ì •ì˜
interface ServerMessage {
  type: string;
  message?: string;
  user_text?: string;
  ai_text?: string;
  audio_url?: string;
  avatar_action?: string;
  conversation_ended?: boolean;
  scenarios?: Record<string, { name: string; description: string }>;
  scenario_name?: string;
}

const PracticeProgress = () => {
  // ìƒíƒœ ê´€ë¦¬
  const [isConnected, setIsConnected] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [isPlaying, setIsPlaying] = useState(false);
  const [statusMessage, setStatusMessage] = useState("ëŒ€ê¸° ì¤‘");
  const [patientName, setPatientName] = useState("í™˜ì");
  const [conversation, setConversation] = useState<
    { speaker: "user" | "ai"; text: string }[]
  >([]);
  const [scenarios, setScenarios] = useState<ServerMessage["scenarios"]>({});
  const [selectedScenario, setSelectedScenario] = useState("");

  // Ref ê´€ë¦¬
  const websocket = useRef<WebSocket | null>(null);
  const audioContext = useRef<AudioContext | null>(null);
  const audioStream = useRef<MediaStream | null>(null);
  const audioSource = useRef<MediaStreamAudioSourceNode | null>(null);
  const audioWorkletNode = useRef<AudioWorkletNode | null>(null);
  const audioPlayer = useRef<HTMLAudioElement | null>(null);
  const audioQueue = useRef<string[]>([]);

  const processAudioQueue = useCallback(async () => {
    if (isPlaying || audioQueue.current.length === 0) {
      return;
    }

    setIsPlaying(true);
    const audioUrl = audioQueue.current.shift();

    if (audioUrl && audioPlayer.current) {
      try {
        const correctedUrl = audioUrl.replace("/static/audio/", "/cache/tts/");
        const response = await fetch(`http://localhost:8000${correctedUrl}`);
        const audioBlob = await response.blob();
        const objectUrl = URL.createObjectURL(audioBlob);
        audioPlayer.current.src = objectUrl;
        audioPlayer.current.play();
        audioPlayer.current.onended = () => {
          URL.revokeObjectURL(objectUrl);
          setIsPlaying(false);
        };
      } catch (error) {
        console.error("Error fetching or playing audio:", error);
        setIsPlaying(false);
      }
    } else {
      setIsPlaying(false);
    }
  }, [isPlaying]);

  // ì›¹ì†Œì¼“ ì—°ê²° ë° í•´ì œ
  const connectWebSocket = useCallback(() => {
    const userId = `user_${Date.now()}`; // ì„ì‹œ ì‚¬ìš©ì ID
    const wsUrl = `ws://localhost:8000/ws/${userId}`;

    if (websocket.current) {
      websocket.current.close();
    }

    const ws = new WebSocket(wsUrl);

    ws.onopen = () => {
      console.log("WebSocket connected");
      setIsConnected(true);
      setStatusMessage("ì„œë²„ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.");
    };

    ws.onmessage = (event) => {
      const message: ServerMessage = JSON.parse(event.data);
      console.log("Received message:", message);
      handleServerMessage(message);
    };

    ws.onclose = () => {
      console.log("WebSocket disconnected");
      setIsConnected(false);
      setIsRecording(false);
      setStatusMessage("ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.");
    };

    ws.onerror = (error) => {
      console.error("WebSocket error:", error);
      setStatusMessage("ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.");
    };

    websocket.current = ws;
  }, []);

  // ì„œë²„ ë©”ì‹œì§€ ì²˜ë¦¬
  const handleServerMessage = (message: ServerMessage) => {
    setStatusMessage(message.message || statusMessage);

    switch (message.type) {
      case "scenario_selection":
        setScenarios(message.scenarios || {});
        break;
      case "scenario_selected":
        setPatientName(message.scenario_name || "í™˜ì");
        break;
      case "voice_response":
        if (message.user_text) {
          setConversation((prev) => [
            ...prev,
            { speaker: "user", text: message.user_text! },
          ]);
        }
        if (message.ai_text) {
          setConversation((prev) => [
            ...prev,
            { speaker: "ai", text: message.ai_text! },
          ]);
        }
        if (message.audio_url) {
          setIsRecording(false); // TTS ìˆ˜ì‹  ì‹œ ë…¹ìŒ ì¤‘ë‹¨
          audioQueue.current.push(message.audio_url);
          processAudioQueue();
        }
        break;
      case "conversation_ended":
        setIsRecording(false);
        // ì„¸ì…˜ ì¢…ë£Œ ì²˜ë¦¬
        break;
    }
  };

  // ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì´ˆê¸°í™”
  const initAudio = useCallback(async () => {
    try {
      if (!audioContext.current) {
        audioContext.current = new AudioContext({ sampleRate: 16000 });
        await audioContext.current.audioWorklet.addModule("/audioProcessor.js");
      }

      if (audioContext.current.state === "suspended") {
        await audioContext.current.resume();
      }

      if (!audioStream.current) {
        audioStream.current = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
          },
        });
      }

      if (!audioSource.current) {
        audioSource.current = audioContext.current.createMediaStreamSource(
          audioStream.current
        );
      }

      if (!audioWorkletNode.current) {
        audioWorkletNode.current = new AudioWorkletNode(
          audioContext.current,
          "audio-processor"
        );

        audioWorkletNode.current.port.onmessage = (event) => {
          if (websocket.current?.readyState === WebSocket.OPEN) {
            websocket.current.send(event.data);
          }
        };
      }
    } catch (error) {
      console.error("Error initializing audio:", error);
      setStatusMessage("ë§ˆì´í¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.");
    }
  }, []);

  // ì‹¤ìŠµ ì‹œì‘/ì¢…ë£Œ í•¸ë“¤ëŸ¬
  const handleToggleRecording = async () => {
    if (!isConnected) {
      connectWebSocket();
      return;
    }

    if (isPlaying) {
      setStatusMessage("í™˜ìê°€ ë§í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...");
      return;
    }

    if (!isRecording) {
      await initAudio();
      setIsRecording(true);
    } else {
      setIsRecording(false);
    }
  };

  // ì‹œë‚˜ë¦¬ì˜¤ ì„ íƒ í•¸ë“¤ëŸ¬
  const handleSelectScenario = (scenarioId: string) => {
    if (websocket.current?.readyState === WebSocket.OPEN) {
      setSelectedScenario(scenarioId);
      const command = { type: "select_scenario", scenario_id: scenarioId };
      websocket.current.send(JSON.stringify(command));
    }
  };

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸/ì–¸ë§ˆìš´íŠ¸ ì‹œ íš¨ê³¼
  useEffect(() => {
    connectWebSocket();
    audioPlayer.current = new Audio();

    return () => {
      websocket.current?.close();
      audioStream.current?.getTracks().forEach((track) => track.stop());
      audioContext.current?.close();
    };
  }, [connectWebSocket]);

  // ë…¹ìŒ ìƒíƒœ ë³€ê²½ ì‹œ íš¨ê³¼
  useEffect(() => {
    if (isRecording && audioSource.current && audioWorkletNode.current) {
      audioSource.current.connect(audioWorkletNode.current);
    } else {
      audioSource.current?.disconnect();
    }
  }, [isRecording]);

  useEffect(() => {
    if (!isPlaying) {
      processAudioQueue();
    }
  }, [isPlaying, processAudioQueue]);

  return (
    <S.Container>
      <S.ControlSection>
        <S.Timer>00:00</S.Timer>
        <S.Button onClick={handleToggleRecording} disabled={!isConnected}>
          {isRecording ? "ì‹¤ìŠµ ì¤‘ì§€" : "ì‹¤ìŠµ ì‹œì‘"}
        </S.Button>
        <S.Button disabled={!isRecording}>ì‹¤ìŠµ ì¢…ë£Œ</S.Button>
        <S.SubmitButton>âœ… ì œì¶œ</S.SubmitButton>
      </S.ControlSection>
      <S.PracticeArea>
        <S.PatientVideoArea>
          <S.PatientAvatar>ğŸ‘¨â€ğŸ’¼</S.PatientAvatar>
          <S.PatientName>{patientName}</S.PatientName>
          <S.StatusBadge>
            {isConnected
              ? isRecording
                ? "ğŸŸ¢ ëŒ€í™” ì¤‘"
                : "ğŸŸ¡ ëŒ€ê¸° ì¤‘"
              : "ğŸ”´ ì—°ê²° ëŠê¹€"}
          </S.StatusBadge>
        </S.PatientVideoArea>
        <S.InfoPanel>
          <S.InfoCard>
            <S.CardHeader>
              <span>ğŸ“‹</span>
              <span>ì‹œë‚˜ë¦¬ì˜¤ ì„ íƒ</span>
            </S.CardHeader>
            <S.InfoGrid>
              {Object.entries(scenarios || {}).map(([id, { name }]) => (
                <S.Button
                  key={id}
                  onClick={() => handleSelectScenario(id)}
                  disabled={isRecording || selectedScenario === id}
                >
                  {name}
                </S.Button>
              ))}
            </S.InfoGrid>
          </S.InfoCard>
          <S.NotesCard>
            <S.CardHeader>
              <span>âœï¸</span>
              <span>ëŒ€í™” ë‚´ìš©</span>
            </S.CardHeader>
            <S.NotesArea>
              {conversation.map((entry, index) => (
                <div key={index}>
                  <strong>
                    {entry.speaker === "user" ? "ë‚˜: " : "í™˜ì: "}
                  </strong>
                  {entry.text}
                </div>
              ))}
            </S.NotesArea>
          </S.NotesCard>
        </S.InfoPanel>
      </S.PracticeArea>
    </S.Container>
  );
};

export default PracticeProgress;
